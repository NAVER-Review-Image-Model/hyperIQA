{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01539453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0782ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import folders\n",
    "\n",
    "class DataLoader(object):\n",
    "    \"\"\"Dataset class for IQA databases\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, path, img_indx, patch_size, patch_num, batch_size=1, istrain=True):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.istrain = istrain\n",
    "\n",
    "        if (dataset == 'live') | (dataset == 'csiq') | (dataset == 'tid2013') | (dataset == 'livec'):\n",
    "            # Train transforms\n",
    "            if istrain:\n",
    "                transforms = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.RandomHorizontalFlip(),\n",
    "                    torchvision.transforms.RandomCrop(size=patch_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                     std=(0.229, 0.224, 0.225))\n",
    "                ])\n",
    "            # Test transforms\n",
    "            else:\n",
    "                transforms = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.RandomCrop(size=patch_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                     std=(0.229, 0.224, 0.225))\n",
    "                ])\n",
    "        elif dataset == 'koniq-10k':\n",
    "            if istrain:\n",
    "                transforms = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.RandomHorizontalFlip(),\n",
    "                    torchvision.transforms.Resize((512, 384)),\n",
    "                    torchvision.transforms.RandomCrop(size=patch_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                     std=(0.229, 0.224, 0.225))])\n",
    "            else:\n",
    "                transforms = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.Resize((512, 384)),\n",
    "                    torchvision.transforms.RandomCrop(size=patch_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                     std=(0.229, 0.224, 0.225))])\n",
    "        elif dataset == 'bid':\n",
    "            if istrain:\n",
    "                transforms = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.RandomHorizontalFlip(),\n",
    "                    torchvision.transforms.Resize((512, 512)),\n",
    "                    torchvision.transforms.RandomCrop(size=patch_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                     std=(0.229, 0.224, 0.225))])\n",
    "            else:\n",
    "                transforms = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.Resize((512, 512)),\n",
    "                    torchvision.transforms.RandomCrop(size=patch_size),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                     std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "        if dataset == 'live':\n",
    "            self.data = folders.LIVEFolder(\n",
    "                root=path, index=img_indx, transform=transforms, patch_num=patch_num)\n",
    "        elif dataset == 'livec':\n",
    "            self.data = folders.LIVEChallengeFolder(\n",
    "                root=path, index=img_indx, transform=transforms, patch_num=patch_num)\n",
    "        elif dataset == 'csiq':\n",
    "            self.data = folders.CSIQFolder(\n",
    "                root=path, index=img_indx, transform=transforms, patch_num=patch_num)\n",
    "        elif dataset == 'koniq-10k':\n",
    "            self.data = folders.Koniq_10kFolder(\n",
    "                root=path, index=img_indx, transform=transforms, patch_num=patch_num)\n",
    "        elif dataset == 'tid2013':\n",
    "            self.data = folders.TID2013Folder(\n",
    "                root=path, index=img_indx, transform=transforms, patch_num=patch_num)\n",
    "        \n",
    "        elif dataset == 'bid':\n",
    "            self.data = folders.CustomDataSet(\n",
    "                root=path, index=img_indx, transform=transforms, patch_num=patch_num)\n",
    "\n",
    "    def get_data(self):\n",
    "        if self.istrain:\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                self.data, batch_size=self.batch_size, shuffle=True)\n",
    "        else:\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                self.data, batch_size=1, shuffle=False)\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd00944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = '../Data/train/'\n",
    "# data_test = '../Data/test/'\n",
    "# data_val = '../Data/valid/'\n",
    "\n",
    "# patch_size = 224\n",
    "\n",
    "# transforms = torchvision.transforms.Compose([\n",
    "#                     torchvision.transforms.RandomHorizontalFlip(),\n",
    "#                     torchvision.transforms.Resize((512, 512)),\n",
    "#                     torchvision.transforms.RandomCrop(size=patch_size),\n",
    "#                     torchvision.transforms.ToTensor(),\n",
    "#                     torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "#                                                      std=(0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12383687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folders\n",
    "\n",
    "# data = folders.CustomDataSet(root=data_train, index=0, transform=transforms, patch_num=25)\n",
    "# data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e068cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class HyperIQASolver(object):\n",
    "    \"\"\"Solver for training and testing hyperIQA\"\"\"\n",
    "    def __init__(self, config, path, train_idx, test_idx, rounds):\n",
    "        self.rounds = rounds\n",
    "\n",
    "        self.epochs = config.epochs\n",
    "        self.test_patch_num = config.test_patch_num\n",
    "\n",
    "        self.model_hyper = models.HyperNet(16, 112, 224, 112, 56, 28, 14, 7).to(device)\n",
    "        self.model_hyper.train(True)\n",
    "\n",
    "        self.l1_loss = torch.nn.L1Loss().to(device)\n",
    "#         self.l1_loss = torch.nn.HuberLoss().to(device)\n",
    "\n",
    "        backbone_params = list(map(id, self.model_hyper.res.parameters()))\n",
    "        self.hypernet_params = filter(lambda p: id(p) not in backbone_params, self.model_hyper.parameters())\n",
    "        self.lr = config.lr\n",
    "        self.lrratio = config.lr_ratio\n",
    "        self.weight_decay = config.weight_decay\n",
    "        paras = [{'params': self.hypernet_params, 'lr': self.lr * self.lrratio},\n",
    "                 {'params': self.model_hyper.res.parameters(), 'lr': self.lr}\n",
    "                 ]\n",
    "        self.solver = torch.optim.Adam(paras, weight_decay=self.weight_decay)\n",
    "\n",
    "        train_loader = DataLoader(config.dataset, config.train_path, train_idx, config.patch_size, config.train_patch_num, batch_size=config.batch_size, istrain=True)\n",
    "        test_loader = DataLoader(config.dataset, config.test_path, test_idx, config.patch_size, config.test_patch_num, istrain=False)\n",
    "        self.train_data = train_loader.get_data()\n",
    "        self.test_data = test_loader.get_data()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Training\"\"\"\n",
    "        best_srcc = 0.0\n",
    "        best_plcc = 0.0\n",
    "        print('Epoch\\tTrain_Loss\\tTrain_SRCC\\tTest_SRCC\\tTest_PLCC\\tTest_MAE\\tTest_MSE')\n",
    "        for t in range(self.epochs):\n",
    "            epoch_loss = []\n",
    "            pred_scores = []\n",
    "            gt_scores = []\n",
    "\n",
    "            for img, label in tqdm(self.train_data):\n",
    "                img = torch.tensor(img.to(device))\n",
    "                label = torch.tensor(label.to(device))\n",
    "\n",
    "                self.solver.zero_grad()\n",
    "\n",
    "                # Generate weights for target network\n",
    "                paras = self.model_hyper(img)  # 'paras' contains the network weights conveyed to target network\n",
    "\n",
    "                # Building target network\n",
    "                model_target = models.TargetNet(paras).to(device)\n",
    "                for param in model_target.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "                # Quality prediction\n",
    "                pred = model_target(paras['target_in_vec'])  # while 'paras['target_in_vec']' is the input to target net\n",
    "                pred_scores = pred_scores + pred.cpu().tolist()\n",
    "                gt_scores = gt_scores + label.cpu().tolist()\n",
    "\n",
    "                loss = self.l1_loss(pred.squeeze(), label.float().detach())\n",
    "                epoch_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "                self.solver.step()\n",
    "\n",
    "            train_srcc, _ = stats.spearmanr(pred_scores, gt_scores)\n",
    "\n",
    "            test_srcc, test_plcc, test_mae, test_mse = self.test(self.test_data)\n",
    "            if test_srcc > best_srcc:\n",
    "                best_srcc = test_srcc\n",
    "                best_plcc = test_plcc\n",
    "            \n",
    "            # save model\n",
    "            weights_file = \"./weight/round_self.rounds{}_epoch_{}.pth\".format(self.rounds, t+1)\n",
    "            torch.save({\n",
    "                'epoch': t,\n",
    "                'model_hyper_state_dict': self.model_hyper.state_dict(),\n",
    "                'model_target_state_dict': model_target.state_dict(),\n",
    "                'optimizer_state_dict': self.solver.state_dict(),\n",
    "                'loss': sum(epoch_loss) / len(epoch_loss)\n",
    "            }, weights_file)\n",
    "\n",
    "            print('%d\\t%4.3f\\t\\t%4.4f\\t\\t%4.4f\\t\\t%4.4f\\t\\t%4.4f\\t\\t%4.4f' %\n",
    "                  (t + 1, sum(epoch_loss) / len(epoch_loss), train_srcc, test_srcc, test_plcc, test_mae, test_mse))\n",
    "\n",
    "            # Update optimizer\n",
    "            lr = self.lr / pow(10, (t // 6))\n",
    "            if t > 8:\n",
    "                self.lrratio = 1\n",
    "            self.paras = [{'params': self.hypernet_params, 'lr': lr * self.lrratio},\n",
    "                          {'params': self.model_hyper.res.parameters(), 'lr': self.lr}\n",
    "                          ]\n",
    "            self.solver = torch.optim.Adam(self.paras, weight_decay=self.weight_decay)\n",
    "\n",
    "        print('Best test SRCC %f, PLCC %f' % (best_srcc, best_plcc))\n",
    "\n",
    "        return best_srcc, best_plcc\n",
    "\n",
    "    def test(self, data):\n",
    "        \"\"\"Testing\"\"\"\n",
    "        self.model_hyper.train(False)\n",
    "        pred_scores = []\n",
    "        gt_scores = []\n",
    "\n",
    "        for img, label in data:\n",
    "            # Data.\n",
    "            img = torch.tensor(img.to(device))\n",
    "            label = torch.tensor(label.to(device))\n",
    "\n",
    "            paras = self.model_hyper(img)\n",
    "            model_target = models.TargetNet(paras).to(device)\n",
    "            model_target.train(False)\n",
    "            pred = model_target(paras['target_in_vec'])\n",
    "\n",
    "            pred_scores.append(float(pred.item()))\n",
    "            gt_scores = gt_scores + label.cpu().tolist()\n",
    "\n",
    "        pred_scores = np.mean(np.reshape(np.array(pred_scores), (-1, self.test_patch_num)), axis=1)\n",
    "        gt_scores = np.mean(np.reshape(np.array(gt_scores), (-1, self.test_patch_num)), axis=1)\n",
    "        test_srcc, _ = stats.spearmanr(pred_scores, gt_scores)\n",
    "        test_plcc, _ = stats.pearsonr(pred_scores, gt_scores)\n",
    "        test_mse = mean_squared_error(pred_scores, gt_scores)\n",
    "        test_mae = mae(pred_scores, gt_scores)\n",
    "\n",
    "        self.model_hyper.train(True)\n",
    "        return test_srcc, test_plcc, test_mae, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "601f115c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "class Config:\n",
    "    dataset = 'bid'\n",
    "    train_patch_num = 3\n",
    "    test_patch_num = 3\n",
    "    lr = 2e-5\n",
    "    weight_decay = 5e-4\n",
    "    lr_ratio = 10\n",
    "    batch_size = 66\n",
    "    epochs = 5\n",
    "    patch_size = 224\n",
    "    train_test_num = 10\n",
    "    train_path = '../Data/train/'\n",
    "    test_path = '../Data/test/'\n",
    "    \n",
    "config = Config()\n",
    "config.train_patch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d152fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7553e286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48007/4071164108.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  srcc_all = np.zeros(config.train_test_num, dtype=np.float)\n",
      "/tmp/ipykernel_48007/4071164108.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  plcc_all = np.zeros(config.train_test_num, dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing on bid dataset for 10 rounds...\n",
      "Round 1\n",
      "Epoch\tTrain_Loss\tTrain_SRCC\tTest_SRCC\tTest_PLCC\tTest_MAE\tTest_MSE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                               | 0/4765 [00:00<?, ?it/s]/tmp/ipykernel_48007/2702538182.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img.to(device))\n",
      "/tmp/ipykernel_48007/2702538182.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label.to(device))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4765/4765 [1:55:37<00:00,  1.46s/it]\n",
      "/tmp/ipykernel_48007/2702538182.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img.to(device))\n",
      "/tmp/ipykernel_48007/2702538182.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label.to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t0.604\t\t0.4762\t\t0.5633\t\t0.5838\t\t0.5370\t\t0.5891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                               | 0/4765 [00:00<?, ?it/s]/tmp/ipykernel_48007/2702538182.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img.to(device))\n",
      "/tmp/ipykernel_48007/2702538182.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label.to(device))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4765/4765 [1:56:13<00:00,  1.46s/it]\n",
      "/tmp/ipykernel_48007/2702538182.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img.to(device))\n",
      "/tmp/ipykernel_48007/2702538182.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label.to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t0.573\t\t0.4911\t\t0.5844\t\t0.6012\t\t0.5236\t\t0.5664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                               | 0/4765 [00:00<?, ?it/s]/tmp/ipykernel_48007/2702538182.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img.to(device))\n",
      "/tmp/ipykernel_48007/2702538182.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label.to(device))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4765/4765 [1:49:11<00:00,  1.37s/it]\n",
      "/tmp/ipykernel_48007/2702538182.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img.to(device))\n",
      "/tmp/ipykernel_48007/2702538182.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label.to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t0.566\t\t0.5009\t\t0.5563\t\t0.5934\t\t0.5251\t\t0.5703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                               | 0/4765 [00:00<?, ?it/s]/tmp/ipykernel_48007/2702538182.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img.to(device))\n",
      "/tmp/ipykernel_48007/2702538182.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label.to(device))\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4765/4765 [1:49:44<00:00,  1.38s/it]\n",
      "/tmp/ipykernel_48007/2702538182.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img.to(device))\n",
      "/tmp/ipykernel_48007/2702538182.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label.to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\t0.563\t\t0.5051\t\t0.5782\t\t0.5825\t\t0.5271\t\t0.5848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                               | 0/4765 [00:00<?, ?it/s]/tmp/ipykernel_48007/2702538182.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img.to(device))\n",
      "/tmp/ipykernel_48007/2702538182.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label.to(device))\n",
      " 38%|███████████████████████████████████████████                                                                       | 1801/4765 [41:34<1:08:25,  1.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     test_index \u001b[38;5;241m=\u001b[39m sel_num[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(sel_num))):\u001b[38;5;28mlen\u001b[39m(sel_num)]\n\u001b[1;32m     12\u001b[0m     solver \u001b[38;5;241m=\u001b[39m HyperIQASolver(config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, train_index, test_index, i)\n\u001b[0;32m---> 13\u001b[0m     srcc_all[i], plcc_all[i] \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(srcc_all)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(plcc_all)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m srcc_med \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(srcc_all)\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mHyperIQASolver.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m gt_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data):\n\u001b[1;32m     50\u001b[0m     img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(img\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     51\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/cuong/hyperIQA/folders.py:365\u001b[0m, in \u001b[0;36mCustomDataSet.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m    364\u001b[0m     path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 365\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample, target\n",
      "File \u001b[0;32m~/cuong/hyperIQA/folders.py:341\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    340\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.8/site-packages/PIL/Image.py:901\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    858\u001b[0m ):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu/lib/python3.8/site-packages/PIL/ImageFile.py:257\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m         )\n\u001b[1;32m    256\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 257\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "srcc_all = np.zeros(config.train_test_num, dtype=np.float)\n",
    "plcc_all = np.zeros(config.train_test_num, dtype=np.float)\n",
    "sel_num = list(range(0, 29))\n",
    "print('Training and testing on %s dataset for %d rounds...' % (config.dataset, config.train_test_num))\n",
    "for i in range(config.train_test_num):\n",
    "    print('Round %d' % (i+1))\n",
    "    # Randomly select 80% images for training and the rest for testing\n",
    "    random.shuffle(sel_num)\n",
    "    train_index = sel_num[0:int(round(0.8 * len(sel_num)))]\n",
    "    test_index = sel_num[int(round(0.8 * len(sel_num))):len(sel_num)]\n",
    "\n",
    "    solver = HyperIQASolver(config, ' ', train_index, test_index, i)\n",
    "    srcc_all[i], plcc_all[i] = solver.train()\n",
    "\n",
    "# print(srcc_all)\n",
    "# print(plcc_all)\n",
    "srcc_med = np.median(srcc_all)\n",
    "plcc_med = np.median(plcc_all)\n",
    "\n",
    "print('Testing median SRCC %4.4f,\\tmedian PLCC %4.4f' % (srcc_med, plcc_med))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72b6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
